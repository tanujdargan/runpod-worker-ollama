{
  "title": "Runpod Worker Ollama",
  "description": "A serverless Ollama Worker for Runpod",
  "type": "serverless",
  "category": "language",
  "iconUrl": "https://ollama.com/public/ollama.png",
  "config": {
    "runsOn": "GPU",
    "gpuCount": 1,
    "gpuIds": "AMPERE_16,AMPERE_24,ADA_24",
    "containerDiskInGb": 20,
    "presets": [],
    "env": [
      {
        "key": "OLLAMA_MODEL_NAME",
        "input": {
          "name": "Model Name",
          "type": "string",
          "description": "Name of a model to preload from Ollama",
          "default": "medgemma:27b",
          "advanced": false
        }
      },
      {
        "key": "MAX_CONCURRENCY",
        "input": {
          "name": "Max Concurrency",
          "type": "number",
          "description": "Maximum number of concurrent requests to handle (default: 8)",
          "default": 8,
          "advanced": true
        }
      },
      {
        "key": "OLLAMA_NUM_PARALLEL",
        "input": {
          "name": "Parallel Requests",
          "type": "string",
          "description": "Maximum number of concurrent requests to handle (default: 4 or 1)",
          "default": "",
          "advanced": true
        }
      },
      {
        "key": "OLLAMA_FLASH_ATTENTION",
        "input": {
          "name": "Flash Attention",
          "type": "string",
          "description": "Enable flash attention for faster inference (set to 1)",
          "default": "1",
          "advanced": true
        }
      },
      {
        "key": "OLLAMA_MODELS",
        "input": {
          "name": "Ollama Models Directory",
          "type": "string",
          "description": "Directory to store Ollama models (default: /runpod-volume for persistence)",
          "default": "/runpod-volume",
          "advanced": true
        }
      }
    ]
  }
}